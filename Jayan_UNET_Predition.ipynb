{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75db6a0-e594-452c-95f6-914823fa8e76",
   "metadata": {},
   "source": [
    "# Predict images using trained UNET and compute entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80204da-63d5-44ab-b063-dca3ad1d4de1",
   "metadata": {},
   "source": [
    "## 1.0 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af97c0a4-7df1-4278-9129-acd41b90f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "import glob\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd19c79-3c65-4c78-9d9a-abc68deacaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchgeo.samplers import RandomGeoSampler, GridGeoSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.datasets import stack_samples\n",
    "from torchgeo.datasets import RasterDataset\n",
    "from torchsummary import summary\n",
    "#import pylab as plt\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b49005-9e8a-479f-9cad-329e92febb74",
   "metadata": {},
   "source": [
    "# 2.0 Load and prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290b6877-d45e-4369-ab65-af44c8baa3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CLASS for images and labels\n",
    "class BengaluruDatasetImages(RasterDataset):\n",
    "    \"\"\"\n",
    "    Load image data that ends in *.tif\n",
    "    \"\"\"\n",
    "\n",
    "    # filename_glob = \"*.tif\"\n",
    "    def __init__(self, root, filename_glob='*.tif', **kwargs):\n",
    "        self.filename_glob = filename_glob\n",
    "        super().__init__(root, **kwargs)\n",
    "    \n",
    "\n",
    "class BengaluruDatasetLabels(RasterDataset):\n",
    "    \"\"\"\n",
    "    Load label data that ends in *.tif\n",
    "    \"\"\"\n",
    "    is_image = False\n",
    "    filename_glob = \"*.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd045625-8cb0-462b-b8fc-0d5352e0cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base path of the dataset\n",
    "TRAIN_PATH = os.path.join(\"Data\", \"Train\")\n",
    "VALID_PATH = os.path.join(\"Data\", \"Valid\")\n",
    "TEST_PATH = os.path.join(\"Data\", \"Test\")\n",
    "#tr_labels = lambda x: x[:,0,:,:].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0c0926-00b2-4ece-9531-2bbcce798b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CLASS for image and label transformation\n",
    "\n",
    "class TransformBengaluruImages(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Min and Max scale to the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs[\"image\"] -= inputs[\"image\"].min()\n",
    "        inputs[\"image\"] /= inputs[\"image\"].max()\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "class TransformBengaluruLabels(nn.Module):\n",
    "    \"\"\"\n",
    "    Create additional dimensions for the labels np arrays\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Batch\n",
    "        if inputs[\"mask\"].ndim == 4:\n",
    "            inputs[\"mask\"] = (inputs[\"mask\"][:,0,:,:]).long()\n",
    "        # Sample\n",
    "        else:\n",
    "            inputs[\"mask\"] = (inputs[\"mask\"][0,:,:]).long()\n",
    "        \n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416cf11b-38ff-4e5b-892d-3e6e0d9fe67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train and Test datasets\n",
    "tr_im = BengaluruDatasetImages(os.path.join(TRAIN_PATH, \"Images\"), transforms=TransformBengaluruImages())\n",
    "tr_la = BengaluruDatasetLabels(os.path.join(TRAIN_PATH, \"Labels\"), transforms=TransformBengaluruLabels())\n",
    "\n",
    "TRAIN_DS = tr_im & tr_la\n",
    "\n",
    "# Create Train and Test datasets\n",
    "vl_im = BengaluruDatasetImages(os.path.join(VALID_PATH, \"Images\"), transforms=TransformBengaluruImages())\n",
    "vl_la = BengaluruDatasetLabels(os.path.join(VALID_PATH, \"Labels\"), transforms=TransformBengaluruLabels())\n",
    "\n",
    "VALID_DS = vl_im & vl_la\n",
    "\n",
    "ts_im = BengaluruDatasetImages(os.path.join(TEST_PATH, \"Images\"), transforms=TransformBengaluruImages())\n",
    "ts_la = BengaluruDatasetLabels(os.path.join(TEST_PATH, \"Labels\"), transforms=TransformBengaluruLabels())\n",
    "\n",
    "TEST_DS = ts_im & ts_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77e5e73-5860-4bc5-a0da-e0faa2c59743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat UNET CLASS\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = self.contract_block(in_channels, 32, 3, 1)\n",
    "        self.conv2 = self.contract_block(32, 64, 3, 1)\n",
    "        self.conv3 = self.contract_block(64, 128, 3, 1)\n",
    "        self.conv4 = self.contract_block(128, 256, 3, 1)\n",
    "\n",
    "        self.upconv4 = self.expand_block(256, 128, 3, 1)\n",
    "        self.upconv3 = self.expand_block(128*2, 64, 3, 1)\n",
    "        self.upconv2 = self.expand_block(64*2, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "\n",
    "        upconv4 = self.upconv4(conv4)\n",
    "        upconv3 = self.upconv3(torch.cat([upconv4, conv3], 1))\n",
    "        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "\n",
    "        return upconv1\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "                            )\n",
    "        return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3525cd9-7218-4e6f-83a0-0797b95adac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (upconv4): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (upconv3): Sequential(\n",
       "    (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (upconv2): Sequential(\n",
       "    (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (upconv1): Sequential(\n",
       "    (0): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(4, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_model = torch.load('UNET/unet_model_128px_100epoches.pt', map_location=torch.device('cpu'))\n",
    "unet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60cb84-000c-41af-ac7b-d1cfb44953c9",
   "metadata": {},
   "source": [
    "## 3.0 Define GEOIMAGE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247e4208-63b1-405e-b3f5-8e01152c56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoImage():\n",
    "    def __init__(self, img_src, n_classes=4):\n",
    "        self.img_src = img_src\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.ds = gdal.Open(self.img_src)\n",
    "        self.band = self.ds.GetRasterBand(1)\n",
    "        self.arr = self.band.ReadAsArray()\n",
    "        \n",
    "        img_src_split = img_src.split('/')\n",
    "        print('/'.join(img_src_split[:-1]))\n",
    "        print(img_src_split[-1])\n",
    "        self.ds2 = BengaluruDatasetImages('/'.join(img_src_split[:-1]), filename_glob=img_src_split[-1], transforms=TransformBengaluruImages())\n",
    "\n",
    "        self.predictions = np.zeros(self.arr.shape[:2], int)\n",
    "        self.probabilities = np.zeros([self.n_classes, *self.arr.shape[:2]], float)\n",
    "        \n",
    "    def create_dataloader(self, size=128, batch_size=128, stride=110):\n",
    "        self.sampler = GridGeoSampler(self.ds2, size=size, stride=stride, roi=None)\n",
    "        dataloader  = DataLoader(self.ds2, batch_size, sampler=self.sampler, collate_fn=stack_samples)\n",
    "        return dataloader\n",
    "    \n",
    "    def predict(self, net, device='cpu', dataloader=None):\n",
    "        if dataloader is None:\n",
    "            dataloader = self.create_dataloader()\n",
    "        \n",
    "        for sample in dataloader:\n",
    "            pred_batch = net(sample[\"image\"]).to(device)\n",
    "\n",
    "            for idx, pred in enumerate(pred_batch):\n",
    "                prob_numpy = pred.detach().numpy()\n",
    "                pred_numpy = np.argmax(prob_numpy, 0)\n",
    "                self.set_predictions(pred_numpy, sample['bbox'][idx])\n",
    "                self.set_probabilities(prob_numpy, sample['bbox'][idx])\n",
    "        \n",
    "    def get_pixel_coordinates(self, bbox):\n",
    "        org_b = self.ds2.bounds\n",
    "        range_x = org_b.maxx - org_b.minx\n",
    "        range_y = org_b.maxy - org_b.miny\n",
    "        img_b = bbox\n",
    "        minx_pi = int(np.round((img_b.minx - org_b.minx) / range_x * self.arr.shape[0]))\n",
    "        #maxx_pi = minx_pi+128\n",
    "        maxx_pi = int(np.round((img_b.maxx - org_b.minx) / range_x * self.arr.shape[0]))\n",
    "        miny_pi = int(np.round((img_b.miny - org_b.miny) / range_y * self.arr.shape[1]))\n",
    "        #maxy_pi = miny_pi+128\n",
    "        maxy_pi = int(np.round((img_b.maxy - org_b.miny) / range_y * self.arr.shape[1]))\n",
    "        \n",
    "        return minx_pi, maxx_pi, miny_pi, maxy_pi\n",
    "    \n",
    "    def set_predictions(self, pred, bbox):\n",
    "        minx_pi, maxx_pi, miny_pi, maxy_pi = self.get_pixel_coordinates(bbox)\n",
    "        self.predictions[minx_pi:maxx_pi, miny_pi:maxy_pi] = np.rot90(pred, -1)\n",
    "    \n",
    "    def set_probabilities(self, prob, bbox):\n",
    "        minx_pi, maxx_pi, miny_pi, maxy_pi = self.get_pixel_coordinates(bbox)\n",
    "        self.probabilities[:, minx_pi:maxx_pi, miny_pi:maxy_pi] = np.rot90(prob, -1, axes=(1,2))\n",
    "    \n",
    "    def get_predictions(self):\n",
    "        return np.rot90(self.predictions)\n",
    "    \n",
    "    def get_probabilities(self):\n",
    "        return np.rot90(self.probabilities, axes=(1,2))\n",
    "    \n",
    "    def write_tif_files(self, name, folder=''):\n",
    "        self.write_geotiff(f'{folder}{name}_pred.tif' , self.get_predictions())\n",
    "        probs = self.get_probabilities()*1000\n",
    "        for c in range(self.n_classes):\n",
    "            self.write_geotiff(f'{folder}{name}_prob_class{c}.tif', probs[c])\n",
    "    \n",
    "    def write_geotiff(self, filename, arr):\n",
    "        if arr.dtype == np.float32:\n",
    "            arr_type = gdal.GDT_Float32\n",
    "        else:\n",
    "            arr_type = gdal.GDT_Int32\n",
    "\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        out_ds = driver.Create(filename, arr.shape[1], arr.shape[0], 1, arr_type)\n",
    "        out_ds.SetProjection(self.ds.GetProjection())\n",
    "        out_ds.SetGeoTransform(self.ds.GetGeoTransform())\n",
    "        band = out_ds.GetRasterBand(1)\n",
    "        band.WriteArray(arr)\n",
    "        band.FlushCache()\n",
    "        band.ComputeStatistics(False)\n",
    "        \n",
    "\n",
    "def read_geotiff(filename):\n",
    "    ds = gdal.Open(filename)\n",
    "    band = ds.GetRasterBand(1)\n",
    "    arr = band.ReadAsArray()\n",
    "    return arr, ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1767586-de12-4463-8934-75fe3d103531",
   "metadata": {},
   "source": [
    "## 4.0 Apply model to images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f4f50-2641-485d-a7b5-a6e541d6997b",
   "metadata": {},
   "source": [
    "### 4.1 List and read WV images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c983a8b-7c7b-429b-b3bb-e980e20ee967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data/Train/Images/WV_refl_B23567_P01.tif', 'Data/Train/Images/WV_refl_B23567_P04.tif', 'Data/Train/Images/WV_refl_B23567_P07.tif', 'Data/Train/Images/WV_refl_B23567_P09.tif', 'Data/Train/Images/WV_refl_B23567_P14.tif', 'Data/Train/Images/WV_refl_B23567_P16.tif', 'Data/Train/Images/WV_refl_B23567_P17.tif', 'Data/Train/Images/WV_refl_B23567_P18.tif', 'Data/Train/Images/WV_refl_B23567_P19.tif']\n"
     ]
    }
   ],
   "source": [
    "image_path = r'Data/Train/Images/WV*.tif'\n",
    "\n",
    "image_list = glob.glob(image_path)\n",
    "image_list = sorted(image_list)\n",
    "print(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec488ae-725c-4aa4-9217-3e1f3afe5f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Train/Images\n",
      "WV_refl_B23567_P01.tif\n",
      "1_P01\n",
      "Data/Train/Images\n",
      "WV_refl_B23567_P04.tif\n",
      "1_P04\n",
      "Data/Train/Images\n",
      "WV_refl_B23567_P07.tif\n",
      "1_P07\n",
      "Data/Train/Images\n",
      "WV_refl_B23567_P09.tif\n",
      "1_P09\n",
      "Data/Train/Images\n",
      "WV_refl_B23567_P14.tif\n",
      "1_P14\n",
      "Data/Train/Images\n",
      "WV_refl_B23567_P16.tif\n",
      "1_P16\n",
      "Data/Train/Images\n",
      "WV_refl_B23567_P17.tif\n",
      "1_P17\n",
      "Data/Train/Images\n",
      "WV_refl_B23567_P18.tif\n",
      "1_P18\n",
      "Data/Train/Images\n",
      "WV_refl_B23567_P19.tif\n",
      "1_P19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44662489528875216,\n",
       " 0.3590839139655868,\n",
       " 0.35541934988930063,\n",
       " 0.3979224458973133,\n",
       " 0.4842752838202053,\n",
       " 0.48635569265786815,\n",
       " 0.47112137627424877,\n",
       " 0.33142849087068815,\n",
       " 0.36622887440403756]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_list = []\n",
    "for im in image_list:\n",
    "    # print(im)\n",
    "    img = GeoImage(im)\n",
    "    img.predict(net=unet_model)\n",
    "    \n",
    "    probs = softmax(img.get_probabilities(), axis=0)\n",
    "    ent = entropy(probs, axis=0).mean()\n",
    "    entropy_list.append(ent)\n",
    "    \n",
    "    tif_name = '1_' + im[-7:-4]\n",
    "    print(tif_name)\n",
    "    \n",
    "    img.write_tif_files(name=tif_name, folder='PRED/First/')\n",
    "    \n",
    "entropy_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
